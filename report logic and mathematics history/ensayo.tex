\documentclass{report} %{article}
\usepackage[spanish]{babel}
\usepackage[latin1]{inputenc}

\title{Computación e Inteligencia Artificial: orígenes y relaciones con la Lógica y las Matemáticas}
\author{Carlos Javier Tacón Fernández} 
\date{\today}

\begin{document}
\maketitle

%\section{Sección introductoria}
%\cite{c1}

\vspace{12pt}
Los orígenes de la inteligencia artificial se remontan al siglo IV a. C. con el filósofo Aristóteles, que es el padre de la Lógica y cuyos escritos, como \textit{Organon} donde se fundamentan todas las bases de la lógica actual, fueron influencia de todas las personas que contribuyeron a la creación del computador. Un poco más tarde, el matemático griego Euclides definió en su libro \textit{Elementos} las bases de la geometría actual aplicando lógica deductiva y creando una de las obras más importantes para el estudio científico en muchas y diferentes ramas.

\vspace{12pt}
Pero no fue hasta el año 1308 cuando se empezó a pensar en formas de automatizar el razonamiento lógico. Ramon Llull, filósofo y teólogo, publica en este año \textit{Ars Generalis Ultima} uno de los pilares del estudio de la lógica y la razón, tomando de Aristóteles el proceso racional y con intención de aplicarlo al cristianismo de forma automática. Para eso diseña una máquina que a partir de unas sentencias, sujetos y predicados indica como salidas verdadero o falso. La máquina estaba diseñada para funcionar de manera mecánica con palancas, diales y engranajes y su objetivo era corroborar de forma lógica que las creencias religiosas cristianas eran correctas e incluso que otras tesis eran incorrectas. Es el primer esbozo de lo que después se conocería como un autómata finito.

\vspace{12pt}
En 1666 el matemático y filósofo Gottfried Leibniz publica \textit{Dissertatio de arte combinatoria} inspirado en los trabajos de Llull, donde intenta encontrar un lenguaje de conceptos universales. Según él, la razón se podría automatizar con un lenguaje de conceptos puros combinado con procesos formales y métodos matemáticos, un lenguaje jeroglífico en el que cada uno de estos sería un átomo lógico. Así en el mundo terminarían las disputas y conflictos generando un ambiente de consenso y paz. Leibniz también es conocido por diseñar una máquina de cálculo automática aunque parece ser que nunca funcionó como debiera, pero propuso los principios del cálculo automatizado. Los objetivos de Llull y Leibniz podrían considerarse ahora las bases de la sociedad contemporánea

\vspace{12pt}
Años más tarde, en 1854 en Inglaterra, George Boole se basa en los conocimientos de Aristóteles: las leyes fundamentales del funcionamiento de la mente humana, cálculo y lógica, y propone que el razonamiento lógico podría resolverse de manera sistemática, al igual que las ecuaciones. El trabajo de Boole después será recogido por Claude Shannon y aplicado a circuitos eléctricos (primero mediante relays y después mediante transistores), lo que dará paso más tarde al desarrollo de la ALU (unidad aritmético lógica).

\vspace{12pt}
El \textit{Entscheidungsproblem} o problema de decisión era el reto de encontrar si existía un algoritmo general que definiera si una fórmula matemática era cierta. Alan Turing (británico) y Alonzo Church (norteamericano), científicos de la computación, responden simultáneamente a esta pregunta en 1936, ambos coinciden en que no es posible. El primero desarrolla la máquina de Turing (hardware) y el segundo el lambda cálculo (software), ambos enfoques eran equivalentes así que con este sistema cualquier cosa que puede codificarse en hardware también puede codificarse en software.

\vspace{12pt}
En 1950, cuando Alan Turing publica \textit{Computing Machinery and Intelligence} los robots ya formaban parte de la cultura popular, en 1927 se estrenaba la famosa película \textit{Metropolis} de Fritz Lang. Pero hasta 1955 no se acuña el término ''Inteligencia Artificial'', cuando John McCarthy (Dartmouth College), Marvin Minsky (Harvard University), Nathaniel Rochester (IBM), y Claude Shannon (Bell Telephone Laboratories) se juntan para realizar unas jornadas dedicadas a la materia, así nace el término y formalmente esta nueva rama del conocimiento.

\vspace{12pt}
Podemos decir que la definición clásica de programar, desde 1940, se entiende como escribir instrucciones precisas que un computador seguirá, en este caso se aplica la lógica deductiva. Pero en la pasada década, con el auge de las técnicas de \textit{Machine Learning} que se basa en la lógica inductiva, los programas aprenden mediante inferencia estadística: a partir de redes neuronales, estadística y datos se obtienen resultados. Por esto se podría decir que programar en el futuro podría ser exponer a estas redes neuronales al mundo y dejarlas aprender y razonar. Lo que empezó como el estudio de la mente humana podría terminar generando nuevas mentes artificiales, que podrían incluso ser superiores a las nuestras. Hace años, el computador HAL uno de los personajes de la película de Stanley Kubrick \textit{2001: A Space Odyssey} (1968) era simplemente ciencia ficción, ahora tenemos asistentes de voz virtuales como Watson de IBM, incluso en nuestro teléfono móvil. Si hace 50 años todo esto parecía imposible, ¿qué nos esperará dentro de otros 50?

\begin{thebibliography}{9}
\bibitem{c1}
  Fernando Cuartero,
  \textit{Ramon Llull, el Ars Magna y la Informática},
  El País,
  2012.
\bibitem{c2}
  Chris Dixon,
  \textit{How Aristotle created the computer},
  The Atlantic,
  2017.
\bibitem{c3}
  Jonathan Gray,
  \textit{Lets us calculate!: Leibniz, Llull and the computational imagination},
  Public Domain Review,
  2016.
\bibitem{c4}
  Gil Press,
  \textit{A very short history of Artificial Intelligence},
  Forbes,
  2016.
\end{thebibliography}
  
\end{document}